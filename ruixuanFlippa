import transformers
import torch
import torch.nn as nn
# import sys
# import pprint
# import json
# import random
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config

p = 1e-10
dtype = torch.float32  # Specify the data type you want to retrieve the number of bits for
bitwidth = torch.finfo(dtype).bits

def error_map(injectee_shape: tuple, dtype_bitwidth: int, device: torch.device) -> torch.Tensor:
    with torch.no_grad():
        error_map = (2 * torch.ones((*injectee_shape, dtype_bitwidth), dtype=torch.int, device=device)) ** torch.arange(0, dtype_bitwidth, dtype=torch.int, device=device).flip(dims=(-1, )).expand((*injectee_shape, dtype_bitwidth))

        filter = (p * nn.functional.dropout(torch.ones_like(error_map, dtype=torch.float, device=device), 1 - p)).int()

        error_map = (filter * error_map).sum(dim=-1).int()
    return error_map

def error_inject(model, num_lim=1000):
    error_maps = {}
    num_count = 0

    for param_name, param in model.named_parameters():

        if "weight" in param_name or "bias" in param_name:

            if num_count >= num_lim:
                break  # Exit the loop if the desired number of weights is reached

            injectee_shape = param.shape

            error_maps[param_name] = error_map(injectee_shape, bitwidth, device)

            error_fin = error_maps[param_name]

            param.data = (param.view(torch.int) ^ error_fin).view(torch.float)

            num_count += 1

config = GPT2Config.from_pretrained('gpt2')
config.gradient_checkpointing = True
gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')

state_dict = gpt2_model.state_dict()  # Get the model's state_dict
# Create an instance of the modified model
modified_model = GPT2LMHeadModel.from_pretrained('gpt2', config=config, state_dict=state_dict)
# Create a tokenizer for the model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# Set the device to run the model on (e.g., 'cuda' if you have a GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Move the model to the specified device
modified_model = modified_model.to(device)
# Set the modified model to evaluation mode
modified_model.eval()

error_inject(modified_model)


# Generate text using the modified model
input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
with torch.no_grad():
    output = modified_model.generate(input_ids, max_length=50, num_return_sequences=1)

# Decode and print the generated text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print("Generated text:")
print(generated_text)






